# 集成学习
## bagging
基学习器之间是并行的
### eg：随机森林
**bagging+特征选择**
**过程**
### 1.采样
从总体样本中进行有放回的采样N次采集到一个样本量为N的样本集，重复采集M个样本集；
### 2.特征选择
### 3.训练树模型
每个测试集并行训练树模型
### 4.综合结果
投票法
## boosting
**过程**
基学习器之间是串行的，采集样本进行训练，然后基于结果再采样，再训练。
### eg1:adaboost（adaptive boosting)
**过程**
### 1.采样
每个样本初始权重是1/n,随机采样一个N个样本的样本集（无放回的）
### 2.训练一个弱学习器
### 3.计算每个样本残差residual
### 4.根据残差重新计算样本权重
#### 5.根据新的样本权重重新抽样
上一轮分错的样本或者说拟合值差的多的样本会被增肌权重，抽中的概率更大
#### 6.进行新一轮树的训练
### gbdt(gradient boosting decision tree)
### 1.采样
### 2.训练一棵回归树
### 3.计算残差
### 4.根据残差去训练新的树（以残差为目标y）
### 5.循环结束后，把所有树的结果加起来
？什么时候循环停止？保证预测值和实际值相等
**参考文档**：https://www.showmeai.tech/article-detail/193
对异常值敏感
### xgboost
### lightgbm

