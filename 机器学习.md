# 集成学习
## bagging
基学习器之间是并行的
### eg：随机森林
**bagging+特征选择**
**过程**
### 1.采样
从总体样本中进行有放回的采样N次采集到一个样本量为N的样本集，重复采集M个样本集；
### 2.特征选择
### 3.训练树模型
每个测试集并行训练树模型
### 4.综合结果
投票法
## boosting
**过程**
基学习器之间是串行的，采集样本进行训练，然后基于结果再采样，再训练。
### eg1:adaboost（adaptive boosting)
**过程**
### 1.采样
每个样本初始权重是1/n,随机采样一个N个样本的样本集（无放回的）
### 2.训练一个弱学习器
### 3.计算每个样本残差residual
### 4.根据残差重新计算样本权重
#### 5.根据新的样本权重重新抽样
上一轮分错的样本或者说拟合值差的多的样本会被增肌权重，抽中的概率更大
#### 6.进行新一轮树的训练
### eg2:gbdt(gradient boosting decision tree)
**过程**
#### 1.采样T(x<sub>1</sub>,x<sub>2</sub>,......x<sub>N</sub>)
#### 2.训练一棵树F(x)，最小化损失函数值。
#### 3.计算残差，作为下一轮的训练目标
#### 4.根据残差去训练新的树（以残差为目标y）
#### 5.循环结束后，把所有树的结果加起来
？什么时候循环停止？限定树的棵数，逼近设定的偏差阈值
**参考文档**：https://www.showmeai.tech/article-detail/193
对异常值敏感
### eg3:xgboost
**与GBDT的区别**
#### （1）目标函数不同
![d14a1b74-e561-4cb7-860e-89886b4f63ad](https://github.com/user-attachments/assets/f9c641af-a6b8-4c98-b65e-1befb5a56de7)
其中：
红色箭头所指向的L 即为损失函数（比如平方损失函数：l(yi,yi)=(yi−yi)2)
红色方框所框起来的是正则项（包括L1正则、L2正则）
红色圆圈所圈起来的为常数项
对于f(x)，XGBoost利用泰勒展开三项，做一个近似。f(x)表示的是其中一颗回归树。

**参考文档**：https://www.cnblogs.com/mantch/p/11164221.html
https://blog.csdn.net/v_JULY_v/article/details/81410574


![43520e72-e0e6-440b-81ea-92c54112dc28](https://github.com/user-attachments/assets/edb9b2ea-3034-4083-8767-3899b0227c70)

正则项函数包含L1正则项：有关树的子节点数目;L2正则项：对叶节点样本权重之和（为了求导方便，加了一个二分之一系数）
#### （2）泰勒展开
对目标函数进行泰勒展开，减少了计算复杂性，更快速有效的接近目标；
**参考文档** https://zhuanlan.zhihu.com/p/142413825
### lightgbm
**与GBDT的区别**
#### (1)GOSS（gradient-based one-side sampling)
解决的问题：adaboost会根据残差修改样本权重，上一轮分错的样本在下一轮训练中的重要性会下降，但是gbdt不适用这种方法采样，可是实际上gradient可以为采样提供有效信息：一个有较小梯度的实例，实例的训练误差也会较小可以被很好的学习到。如果直接忽略这些实例会影响数据分布，所以想出了gradient-based one-side sampling.
具体做法是：根据梯度绝对值排序，选择topa*100%实例，然后再剩下的数据里随机采样b*100%,然后计算信息增益时，小梯度的样本数据乘常数1-a/b进行放大。这样即可以更多的关注训练不足的样本又可以不改变原始的数据分布了。
#### (2)EFB(exclusive features bunding)
高纬度的数据往往是很稀疏的，特征空间的稀疏性质提供我们设计更小损失的方法去减少特征数量的可能性。尤其在一个稀疏的特征空间按里，很多的特征是互斥，他们不会连续的都是0，我们可以绑定互斥的特征为单一特征。通过精心设计的特征扫描算法，我们可以和从单个特征建立的特征直方图一样的建立特征包中的特征直方图。这样，直方图建立的复杂性从O(num(data)*#num(feature))变成了O(num(data)*num(bundle)),bundle数量远小于特征数量。接下来具体介绍如何实现的细节。
两个问题：第一个问题是决定哪个特征应该被绑在一起，第二个问题是如何构建这些捆绑的特征。
##### 把特征分区到互斥的特征绑定时一个np-hard问题
证明：我们将减少

# 样本选择
## 正负样本比例不协调
### 1.平衡数据集
#### （1）上采样
增加样本数较少的类别的样本，方式是直接复制原来的样本。一般在样本少的类别样本量很小的情况下；
#### （2）下采样
减少样本量较大的类别的样本，丢弃多余的样本，样本大的类别样本量较多的情况下。
#### (3)构造新的样本
##### 基于样本变换的数据增强
样本变换数据增强即采用预设的数据变换规则进行已有数据的扩增，包含单样本数据增强和多样本数据增强。单样本增强(主要用于图像)：主要有几何操作、颜色变换、随机擦除、添加噪声等方法产生新的样本，可参见imgaug开源库。
##### 多样本增强
### 2.修改指标
#### （1）修改正负样本的惩罚权重
模型中有参数设置
#### （2）尝试不同的优化指标
### 3.模型选择
选择对样本不均衡不敏感的模型
这类方法简单来说，通过重复组合少数类样本与抽样的同样数量的多数类样本，训练若干的分类器进行集成学习。
#### （1）采样加集成学习
BalanceCascade BalanceCascade基于Adaboost作为基分类器，核心思路是在每一轮训练时都使用多数类与少数类数量上相等的训练集，然后使用该分类器对全体多数类进行预测，通过控制分类阈值来控制FP（False Positive）率，将所有判断正确的类删除，然后进入下一轮迭代继续降低多数类数量。
EasyEnsemble EasyEnsemble也是基于Adaboost作为基分类器，就是将多数类样本集随机分成 N 个子集，且每一个子集样本与少数类样本相同，然后分别将各个多数类样本子集与少数类样本进行组合，使用AdaBoost基分类模型进行训练，最后bagging集成各基分类器，得到最终模型。示例代码可见：www.kaggle.com/orange90/ensemble-test-credit-score-model-example
通常，在数据集噪声较小的情况下，可以用BalanceCascade，可以用较少的基分类器数量得到较好的表现（基于串行的集成学习方法，对噪声敏感容易过拟合）。噪声大的情况下，可以用EasyEnsemble，基于串行+并行的集成学习方法，bagging多个Adaboost过程可以抵消一些噪声影响。此外还有RUSB、SmoteBoost、balanced RF等其他集成方法可以自行了解。
#### （2）按照异常检测处理
：基于聚类的方法、基于统计的方法、基于深度的方法(孤立森林)、基于分类模型（one-class SVM）以及基于神经网络的方法（自编码器AE）等等。
**参考文档** https://www.cvmart.net/community/detail/6000
