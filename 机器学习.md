# 集成学习
## bagging
基学习器之间是并行的
### eg：随机森林
**bagging+特征选择**
**过程**
### 1.采样
从总体样本中进行有放回的采样N次采集到一个样本量为N的样本集，重复采集M个样本集；
### 2.特征选择
### 3.训练树模型
每个测试集并行训练树模型
### 4.综合结果
投票法
## boosting
**过程**
基学习器之间是串行的，采集样本进行训练，然后基于结果再采样，再训练。
### eg1:adaboost（adaptive boosting)
**过程**
### 1.采样
每个样本初始权重是1/n,随机采样一个N个样本的样本集（无放回的）
### 2.训练一个弱学习器
### 3.计算每个样本残差residual
### 4.根据残差重新计算样本权重
#### 5.根据新的样本权重重新抽样
上一轮分错的样本或者说拟合值差的多的样本会被增肌权重，抽中的概率更大
#### 6.进行新一轮树的训练
### eg2:gbdt(gradient boosting decision tree)
**过程**
#### 1.采样T(x<sub>1</sub>,x<sub>2</sub>,......x<sub>N</sub>)
#### 2.训练一棵树F(x)，最小化损失函数值。
#### 3.计算残差，作为下一轮的训练目标
#### 4.根据残差去训练新的树（以残差为目标y）
#### 5.循环结束后，把所有树的结果加起来
？什么时候循环停止？限定树的棵数，逼近设定的偏差阈值
**参考文档**：https://www.showmeai.tech/article-detail/193
对异常值敏感
### eg3:xgboost
**与GBDT的区别**
#### （1）目标函数增加正则项
修复的GBDT问题：控制模型复杂度，防止过拟合

![43520e72-e0e6-440b-81ea-92c54112dc28](https://github.com/user-attachments/assets/edb9b2ea-3034-4083-8767-3899b0227c70)

正则项函数包含L1正则项：有关树的子节点数目;L2正则项：对叶节点样本权重之和（为了求导方便，加了一个二分之一系数）
#### （2）泰勒展开
对目标函数进行泰勒展开，减少了计算复杂性，更快速有效的接近目标；
**参考文档** https://zhuanlan.zhihu.com/p/142413825
### lightgbm
**与GBDT的区别**
#### （1）GOSS（gradient-based one-side sampling)
解决的问题：adaboost会根据残差修改样本权重，上一轮分错的样本在下一轮训练中的重要性会下降，但是gbdt不适用这种方法采样，可是实际上gradient可以为采样提供有效信息：一个有较小梯度的实例，实例的训练误差也会较小可以被很好的学习到。如果直接忽略这些实例会影响数据分布，所以想出了gradient-based one-side sampling.
具体做法是：根据梯度绝对值排序，选择topa*100%实例，然后再剩下的数据里随机采样b*100%,然后计算信息增益时，小梯度的样本数据乘常数1-a/b进行放大。这样即可以更多的关注训练不足的样本又可以不改变原始的数据分布了。



